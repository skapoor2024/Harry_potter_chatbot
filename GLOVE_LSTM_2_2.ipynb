{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ervjSv-TpTfc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Attention, TimeDistributed,LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "3QvJkpbetnE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "eOWvE5xPD22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "XriXP9MEfUxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "e531SsBzb8Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_glove_embeddings(path, word_index, embedding_dim=100):\n",
        "    embeddings_index = {}\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector  # words not found in the embedding index will be all-zeros.\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "pk2N_gCJpfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Extract dialogues and preprocess\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Append the preprocessed lines to the lists\n",
        "                inputs.append(input_text)\n",
        "                targets.append('<start>' + response_text + '<end>')\n",
        "\n",
        "    #print(\"Sample input with tokens:\", inputs[0]\n",
        "    #print(\"Sample target with tokens:\", targets[0])\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(inputs + targets)\n",
        "    if '<start>' not in tokenizer.word_index:\n",
        "        tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
        "    if '<end>' not in tokenizer.word_index:\n",
        "        tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n",
        "    print(\"Index of '<start>':\", tokenizer.word_index.get('<start>'))\n",
        "    print(\"Index of '<end>':\", tokenizer.word_index.get('<end>'))\n",
        "    input_seqs = tokenizer.texts_to_sequences(inputs)\n",
        "    target_seqs = tokenizer.texts_to_sequences(targets)\n",
        "    max_len = max(max([len(seq) for seq in input_seqs]), max([len(seq) for seq in target_seqs]))\n",
        "    encoder_input_data = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
        "    decoder_input_data = pad_sequences(target_seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    num_classes = len(tokenizer.word_index) + 1\n",
        "    # One-hot encode the target sequences\n",
        "    decoder_target_data = np.zeros((len(decoder_input_data), max_len, num_classes), dtype='float32')\n",
        "    for i in range(len(decoder_input_data)):\n",
        "        for j in range(max_len):\n",
        "            if j < len(decoder_input_data[i]):\n",
        "                decoder_target_data[i, j, decoder_input_data[i][j]] = 1\n",
        "    print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
        "    print(\"Shape of decoder input data:\", decoder_input_data.shape)\n",
        "    print(\"Shape of decoder target data:\", decoder_target_data.shape)\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data, tokenizer, max_len\n"
      ],
      "metadata": {
        "id": "i8WYj9xbpss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_inference(data,tokenizer):\n",
        "    # Extract dialogues and preprocess\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Append the preprocessed lines to the lists\n",
        "                inputs.append(input_text)\n",
        "                targets.append('<start>' + response_text + '<end>')\n",
        "\n",
        "\n",
        "    input_seqs = tokenizer.texts_to_sequences(inputs)\n",
        "    target_seqs = tokenizer.texts_to_sequences(targets)\n",
        "    max_len = max(max([len(seq) for seq in input_seqs]), max([len(seq) for seq in target_seqs]))\n",
        "    encoder_input_data = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
        "    decoder_input_data = pad_sequences(target_seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    num_classes = len(tokenizer.word_index) + 1\n",
        "    # One-hot encode the target sequences\n",
        "    decoder_target_data = np.zeros((len(decoder_input_data), max_len, num_classes), dtype='float32')\n",
        "    for i in range(len(decoder_input_data)):\n",
        "        for j in range(max_len):\n",
        "            if j < len(decoder_input_data[i]):\n",
        "                decoder_target_data[i, j, decoder_input_data[i][j]] = 1\n",
        "    print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
        "    print(\"Shape of decoder input data:\", decoder_input_data.shape)\n",
        "    print(\"Shape of decoder target data:\", decoder_target_data.shape)\n",
        "\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data, max_len"
      ],
      "metadata": {
        "id": "ZGVye5DBU_FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(encoder_input_data, decoder_input_data, decoder_target_data, batch_size=64):\n",
        "    # Create a TensorFlow dataset object\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"encoder_inputs\": encoder_input_data, \"decoder_inputs\": decoder_input_data},\n",
        "        decoder_target_data\n",
        "    ))\n",
        "\n",
        "    # Cache the dataset, shuffle, batch, and prefetch to improve performance\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(buffer_size=len(encoder_input_data))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6xPvjhUO2FXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, max_len, embedding_matrix):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,),name = 'encoder_inputs')\n",
        "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
        "\n",
        "    # Two BiLSTM Encoder layers\n",
        "    encoder_bilstm1 = Bidirectional(LSTM(128, return_sequences=True, return_state=True, dropout=0.2))\n",
        "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bilstm1(encoder_embedding)\n",
        "    state_h1 = Concatenate()([forward_h1, backward_h1])\n",
        "    state_c1 = Concatenate()([forward_c1, backward_c1])\n",
        "\n",
        "    encoder_bilstm2 = Bidirectional(LSTM(128, return_sequences=True, return_state=True, dropout=0.2))\n",
        "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bilstm2(encoder_outputs1)\n",
        "    state_h2 = Concatenate()([forward_h2, backward_h2])\n",
        "    state_c2 = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,), name = 'decoder_inputs')\n",
        "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, dropout=0.2)\n",
        "    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=[state_h2, state_c2])\n",
        "\n",
        "    # Self-Attention Layer\n",
        "    attention_layer = Attention()\n",
        "    attention_result = attention_layer([decoder_outputs, encoder_outputs2])\n",
        "\n",
        "    # Concatenate attention input and decoder LSTM output\n",
        "    decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_result])\n",
        "    decoder_concat_input = LayerNormalization()(decoder_concat_input)\n",
        "\n",
        "    # Dense layer for output\n",
        "    decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "VlzBeIlytrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "train_path = '/content/drive/My Drive/en_train_set.json'\n",
        "train_data = load_data(train_path)\n",
        "encoder_input_data, decoder_input_data,decoder_target_data, tokenizer, max_sequence_len = preprocess_data(train_data)\n",
        "train_encoder_input, val_encoder_input, train_decoder_input, val_decoder_input, train_decoder_target, val_decoder_target = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2, random_state=13)\n",
        "# Create training dataset\n",
        "train_dataset = create_dataset(train_encoder_input, train_decoder_input, train_decoder_target, batch_size=8)\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = create_dataset(val_encoder_input, val_decoder_input, val_decoder_target, batch_size=8)"
      ],
      "metadata": {
        "id": "mVDu4Hkzt3nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d308407d-d3fc-485c-b7d1-7682265da8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of '<start>': 8005\n",
            "Index of '<end>': 8006\n",
            "Shape of encoder input data: (4372, 600)\n",
            "Shape of decoder input data: (4372, 600)\n",
            "Shape of decoder target data: (4372, 600, 8007)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = '/content/drive/My Drive/glove.6B.100d.txt'\n",
        "embedding_matrix = get_glove_embeddings(glove_path, tokenizer.word_index)"
      ],
      "metadata": {
        "id": "pOCQFHVSuaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(len(tokenizer.word_index) + 1,max_sequence_len, embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1Q41B9ut4P",
        "outputId": "31d76325-07f7-420c-8f53-381d84871ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, None, 100)            800700    ['encoder_inputs[0][0]']      \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirecti  [(None, None, 256),          234496    ['embedding_4[0][0]']         \n",
            " onal)                        (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirecti  [(None, None, 256),          394240    ['bidirectional_4[0][0]']     \n",
            " onal)                        (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, None, 100)            800700    ['decoder_inputs[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 256)                  0         ['bidirectional_5[0][1]',     \n",
            " e)                                                                  'bidirectional_5[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 256)                  0         ['bidirectional_5[0][2]',     \n",
            " e)                                                                  'bidirectional_5[0][4]']     \n",
            "                                                                                                  \n",
            " lstm_8 (LSTM)               (None, None, 256)            365568    ['embedding_5[0][0]',         \n",
            "                                                                     'concatenate_12[0][0]',      \n",
            "                                                                     'concatenate_13[0][0]']      \n",
            "                                                                                                  \n",
            " attention_2 (Attention)     (None, None, 256)            0         ['lstm_8[0][0]',              \n",
            "                                                                     'bidirectional_5[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, None, 512)            0         ['lstm_8[0][0]',              \n",
            " e)                                                                  'attention_2[0][0]']         \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, None, 512)            1024      ['concatenate_14[0][0]']      \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " time_distributed_2 (TimeDi  (None, None, 8007)           4107591   ['layer_normalization_2[0][0]'\n",
            " stributed)                                                         ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6704319 (25.57 MB)\n",
            "Trainable params: 5102919 (19.47 MB)\n",
            "Non-trainable params: 1601400 (6.11 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "qnfUJ2BgfI_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VlhlpIjSVgUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEUScoreCallback(Callback):\n",
        "    def __init__(self, tokenizer, encoder_input_train, decoder_target_train, encoder_input_val, decoder_target_val, max_len):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.encoder_input_train = encoder_input_train\n",
        "        self.decoder_target_train = decoder_target_train\n",
        "        self.encoder_input_val = encoder_input_val\n",
        "        self.decoder_target_val = decoder_target_val\n",
        "        self.max_len = max_len\n",
        "        self.start_token_index = tokenizer.word_index['<start>']\n",
        "        self.end_token_index = tokenizer.word_index['<end>']\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_bleu = self.calculate_bleu(self.encoder_input_train, self.decoder_target_train, 'Train')\n",
        "        val_bleu = self.calculate_bleu(self.encoder_input_val, self.decoder_target_val, 'Validation')\n",
        "        print(f'Epoch {epoch + 1}: Train BLEU Score = {train_bleu:.4f}, Validation BLEU Score = {val_bleu:.4f}')\n",
        "\n",
        "    def calculate_bleu(self, encoder_input_data, decoder_target_data, data_type):\n",
        "        total_bleu_score = 0\n",
        "        num_samples = int(len(encoder_input_data)/10)\n",
        "        sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "        for idx in sample_indices:\n",
        "            input_seq = encoder_input_data[idx:idx+1]  # Process one sequence at a time\n",
        "            target_seq = [self.start_token_index]\n",
        "            output_sentence = []\n",
        "\n",
        "            for _ in range(self.max_len):\n",
        "                target_seq_array = np.array([target_seq])\n",
        "                predictions = self.model.predict([input_seq, target_seq_array])\n",
        "                sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "                if sampled_token_index == self.end_token_index:\n",
        "                    break\n",
        "                output_sentence.append(sampled_token_index)\n",
        "                target_seq.append(sampled_token_index)\n",
        "\n",
        "            # Convert sequences to text\n",
        "            reference = self.tokenizer.sequences_to_texts([decoder_target_data[idx]])\n",
        "            candidate = self.tokenizer.sequences_to_texts([output_sentence])\n",
        "            bleu_score = sentence_bleu([reference[0].split()], candidate[0].split())\n",
        "            total_bleu_score += bleu_score\n",
        "\n",
        "        average_bleu_score = total_bleu_score / num_samples\n",
        "        return average_bleu_score"
      ],
      "metadata": {
        "id": "xXu6oJ-jdPgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bleu_callback = BLEUScoreCallback(tokenizer, train_encoder_input, train_decoder_target, val_encoder_input, val_decoder_target, max_sequence_len)\n",
        "model_history = model.fit(train_dataset, epochs=5,validation_data = val_dataset,verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD_2nswjvvE4",
        "outputId": "379484fa-c5fc-496b-dba6-f73db8f2887c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "55/55 [==============================] - 36s 522ms/step - loss: 0.8649 - accuracy: 0.9621 - val_loss: 0.1278 - val_accuracy: 0.9810\n",
            "Epoch 2/5\n",
            "55/55 [==============================] - 27s 491ms/step - loss: 0.1266 - accuracy: 0.9808 - val_loss: 0.1234 - val_accuracy: 0.9822\n",
            "Epoch 3/5\n",
            "55/55 [==============================] - 27s 491ms/step - loss: 0.1250 - accuracy: 0.9812 - val_loss: 0.1170 - val_accuracy: 0.9824\n",
            "Epoch 4/5\n",
            "55/55 [==============================] - 27s 492ms/step - loss: 0.1186 - accuracy: 0.9817 - val_loss: 0.1149 - val_accuracy: 0.9826\n",
            "Epoch 5/5\n",
            "55/55 [==============================] - 27s 492ms/step - loss: 0.1162 - accuracy: 0.9819 - val_loss: 0.1134 - val_accuracy: 0.9829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to save the model weights in the drive\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/model_glove_bilstm_weights.h5')\n"
      ],
      "metadata": {
        "id": "pjTRVcbTwX0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1soWw_r-8zwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(model_history.history['loss'], label='Training Loss')\n",
        "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(model_history.history['accuracy'], label='Training Accuracy')  # Change 'accuracy' as per your metrics\n",
        "# plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.legend()\n",
        "# plt.title('Accuracy Over Epochs')\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "DV20UacbTRJA",
        "outputId": "bd5506b5-5839-4c80-a490-1f9b50001346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Over Epochs')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "zfseckq1CU9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the test dataset"
      ],
      "metadata": {
        "id": "Wvikcl52YPi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses_and_evaluate_bleu(model, tokenizer, encoder_input_data, decoder_target_data, max_len, start_token_index, end_token_index):\n",
        "    total_bleu_score = 0\n",
        "    num_samples = int(len(encoder_input_data)/100)  # Use the entire dataset\n",
        "    sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "    total_time = 0\n",
        "    response_times = []\n",
        "\n",
        "\n",
        "\n",
        "    all_inputs = []\n",
        "    all_references = []\n",
        "    all_candidates = []\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        input_seq = encoder_input_data[idx:idx+1]  # Extract one sequence for further processing\n",
        "        input_text = tokenizer.sequences_to_texts(input_seq)[0]  # Convert the extracted sequence to text\n",
        "        target_seq = [start_token_index]\n",
        "        output_sentence = []\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"sentence_started: \",input_text)\n",
        "        print(input_text)\n",
        "        print(\"Harry: \",end = ' ')\n",
        "        for _ in range(max_len):\n",
        "            target_seq_array = np.array([target_seq])\n",
        "            predictions = model.predict([input_seq, target_seq_array],verbose = 0)\n",
        "            sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "            if sampled_token_index == end_token_index or len(target_seq) == max_len:\n",
        "                break\n",
        "            sampled_word = tokenizer.index_word[sampled_token_index] if sampled_token_index in tokenizer.index_word else ''\n",
        "            print(sampled_word,end = ' ')  # Print each predicted word\n",
        "            output_sentence.append(sampled_token_index)\n",
        "            target_seq.append(sampled_token_index)\n",
        "\n",
        "        print()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "        response_times.append(elapsed_time)\n",
        "        print(\"time elapsed: \",elapsed_time)\n",
        "\n",
        "        # Convert sequences to text\n",
        "        reference = tokenizer.sequences_to_texts([decoder_target_data[idx]])\n",
        "        candidate = tokenizer.sequences_to_texts([output_sentence])\n",
        "\n",
        "        all_inputs.append(input_text[0].split())\n",
        "        all_references.append(reference[0].split())\n",
        "        all_candidates.append(candidate[0].split())\n",
        "\n",
        "        # Calculate individual BLEU score\n",
        "        bleu_score = sentence_bleu([reference[0].split()], candidate[0].split())\n",
        "        total_bleu_score += bleu_score\n",
        "\n",
        "        # print(\"Input Sentence:\", input_text)\n",
        "        # print(\"Model Output:\", candidate)\n",
        "        # print(\"Reference Sentence:\", reference)\n",
        "        # print(\"---\" * 10)  # Separator for readability\n",
        "        print(\"sentence_finished\")\n",
        "\n",
        "\n",
        "    average_bleu_score = total_bleu_score / num_samples\n",
        "    average_response_time = sum(response_times) / num_samples\n",
        "\n",
        "    return all_inputs,all_references,all_candidates,response_times,average_bleu_score,average_response_time\n"
      ],
      "metadata": {
        "id": "jsQBnqh69txy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = load_data('/content/drive/My Drive/en_test_set.json')\n",
        "encoder_input_data, decoder_input_data, decoder_target_data, max_len = preprocess_data_inference(test_data,tokenizer)"
      ],
      "metadata": {
        "id": "Yp9KwFVKYfM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses(model, tokenizer, encoder_input_data, max_len, start_token_index, end_token_index):\n",
        "    num_samples = len(encoder_input_data)# Reduce the dataset for quicker response times\n",
        "    sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "    total_time = 0\n",
        "    response_times = []\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        input_seq = encoder_input_data[idx:idx+1]  # Extract one sequence for further processing\n",
        "        input_text = tokenizer.sequences_to_texts(input_seq)[0]  # Convert the extracted sequence to text\n",
        "\n",
        "        print(\"Input: \", input_text)\n",
        "        print(\"Harry: \", end=' ')\n",
        "\n",
        "        target_seq = [start_token_index]\n",
        "        output_sentence = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        while len(target_seq) < max_len:\n",
        "            target_seq_array = np.array([target_seq])\n",
        "            predictions = model.predict([input_seq, target_seq_array], verbose=0)\n",
        "            sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "            if sampled_token_index == end_token_index or len(target_seq) == max_len:\n",
        "                break\n",
        "\n",
        "            sampled_word = tokenizer.index_word.get(sampled_token_index, '')\n",
        "            print(sampled_word, end=' ')  # Print each predicted word on the same line\n",
        "            output_sentence.append(sampled_token_index)\n",
        "            target_seq.append(sampled_token_index)\n",
        "\n",
        "        print()  # New line after the end of response\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "        response_times.append(elapsed_time)\n",
        "\n",
        "        print(\"Time elapsed for response: {:.4f} seconds\".format(elapsed_time))\n",
        "        print(\"---\" * 20)  # Separator for readability\n",
        "\n",
        "    average_response_time = total_time / num_samples\n",
        "\n",
        "    return average_response_time, num_samples\n"
      ],
      "metadata": {
        "id": "ylTkuECQXwHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "average_response_time, num_samples = generate_responses(\n",
        "    model, tokenizer, encoder_input_data, 100,\n",
        "    tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
        ")\n",
        "print(\"Processed {} sentences with an average response time of {:.4f} seconds.\".format(num_samples, average_response_time))"
      ],
      "metadata": {
        "id": "y7x5QyujX_DJ",
        "outputId": "e5c0e9dc-f930-4922-842c-ad0d3d7a4530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  are you all right you look awful\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 10.9827 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  nah that just one of those things you tell kids to teach them lessons isnt it dont go looking for trouble dont pick fights dont go messing around with stuff thats best left alone just keep your head down mind your own business and youll be okay  come to think of it maybe that why elder wands are supposed to be unlucky\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3497 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  good afternoon\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3134 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  merry christmas\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3315 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  you were yelling your head off\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.2997 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  stan shunpike\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.2816 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  i think that if you choose to return there is a chance that he may be finished for good i cannot promise it but i know this harry that you have less to fear from returning here than he does harry pity the living and above all those who live without love by returning you may ensure that souls are maimed families are torn apart if that seems to you a worthy goal then we say good bye for the present\n",
            "Harry:  i end                                                 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-913413c046b4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m average_response_time, num_samples = generate_responses(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-41-268f67212800>\u001b[0m in \u001b[0;36mgenerate_responses\u001b[0;34m(model, tokenizer, encoder_input_data, max_len, start_token_index, end_token_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtarget_seq_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_truncate_execution_to_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         should_truncate = (\n\u001b[1;32m   1360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m         \u001b[0moriginal_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \"\"\"\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    809\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    810\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    535\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs,test_references,test_candidates,test_times,bleu_score,avg_time = generate_responses_and_evaluate_bleu(\n",
        "    model, tokenizer, encoder_input_data, decoder_target_data, max_len,\n",
        "    tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
        ")"
      ],
      "metadata": {
        "id": "riMAam1u-0Yr",
        "outputId": "c70150e1-7511-431b-c309-06a137adfa14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_started:  gone\n",
            "gone\n",
            "Harry:  yeah end                                                                                                                                                                                                                   \n",
            "time elapsed:  19.294798374176025\n",
            "sentence_finished\n",
            "sentence_started:  third time this week if you cant control that owl itll have to go\n",
            "third time this week if you cant control that owl itll have to go\n",
            "Harry:  yeah "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end                                                                                                                                                                                                                   \n",
            "time elapsed:  15.826366424560547\n",
            "sentence_finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(len(train_decoder_target), 100)):  # Ensure not to exceed the length of the data or 100 items\n",
        "    # Ensure we pass a list of sequences, even if it's a list with only one sequence\n",
        "    sequence_text = tokenizer.sequences_to_texts([val_decoder_target[i]])\n",
        "    print(sequence_text)  # This will print the list containing the text representation of the i-th sequence\n"
      ],
      "metadata": {
        "id": "eAI17_zSKCoF",
        "outputId": "e091ab94-872e-46f3-f63c-95a9904e79e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['where is it now end']\n",
            "['what about dumbledore hagrid end']\n",
            "['what  end']\n",
            "['yeah i do malfoys fathers in azkaban dont you think malfoyd like revenge end']\n",
            "['fleur didnt turn up i couldnt leave her end']\n",
            "['but if voldemorts trying to recruit more death eaters its bound to get out that hes come back isnt it end']\n",
            "['there were witnesses who saw pettigrew die a whole street full of them end']\n",
            "['oh i dunno lets say i dreamed i was drowning snape in my cauldron yeah thatll do end']\n",
            "['yeah i expect thats what mcgonagall will say when i ask for permission end']\n",
            "['less than zero better try though hadnt i ill offer to do two more detentions or something i dunno i hope she doesnt keep me too long this evening you realize weve got to write three essays practice vanishing spells for mcgonagall work out a countercharm for flitwick finish the bowtruckle drawing and start that stupid dream diary for trelawney end']\n",
            "['fortuna major end']\n",
            "['well hurry up i cant breathe  end']\n",
            "['where are we going end']\n",
            "[' i meant to and thats what did it ive done what my mother did theyre protected from you havent you noticed how none of the spells you put on them are binding you cant torture them you cant touch them you dont learn from your mistakes riddle do you end']\n",
            "['so you think that dream did it really happen end']\n",
            "['kind of makes you wish we had norbert back doesnt it end']\n",
            "['but the fake sword isnt the only thing in that vault is it perhaps youve seen the other things in there end']\n",
            "['it wasnt like that end']\n",
            "['but how did you get in there you need to speak parseltongue end']\n",
            "['i need you two as well end']\n",
            "['dumbledore just said  just said we could save more than one innocent life hermione were going to save buckbeak end']\n",
            "['looks like a really good plan from where im standing theyll just have to stand back and let the dragons do their stuff end']\n",
            "['have you been spying on him too end']\n",
            "['cho women what did she want to talk about cedric for anyway why does she always want to drag up a subject that makes her act like a human hosepipe end']\n",
            "['it looks like kings cross station except a lot cleaner and empty and there are no trains as far as i can see end']\n",
            "['no it definitely didnt sound like an elf end']\n",
            "['hes not looking too good is he end']\n",
            "['and do you  end']\n",
            "['double divination this afternoon end']\n",
            "['the snitch i caught in my first ever quidditch match dont you remember end']\n",
            "['no no end']\n",
            "['like voldemort put on the stone basin in the cave end']\n",
            "['well i dont know what they are but apparently he and his daughter go on holiday looking for them thats her end']\n",
            "['neville end']\n",
            "['ron  end']\n",
            "['no you didnt you didnt do that you cant have done end']\n",
            "['think of a dream quick in case the old toad comes our way end']\n",
            "['whats dumbledore asked you to do hagrid he sent professor mcgonagall to ask you and madame maxime to meet him  that night end']\n",
            "['you considered smith end']\n",
            "['okay end']\n",
            "['i dunno maybe its better when you do it yourself i didnt enjoy it much when dumbledore took me along for the ride charlie failed though didnt he end']\n",
            "['what did you tell her end']\n",
            "['hes friends with that dog ive seen them together come on  and keep your wand out  end']\n",
            "['yeah i want to play quidditch hang on ill get my firebolt end']\n",
            "['unless he was asleep said harry but he still held his breath as hermione knelt down in front of the empty canvas her wand directed at its center cleared her throat then said er  phineas phineas nigellus  end']\n",
            "['they brought the kids from the orphanage here end']\n",
            "['why dyou wear that thing dobby end']\n",
            "['hagrid thanks end']\n",
            "['well im glad he left if he hadnt i wouldnt have done magic and dumbledore would probably have left me at privet drive all summer end']\n",
            "['i hoped hed get back to me quickly end']\n",
            "['he could stillve kept me informed if hed wanted to youre not telling me he doesnt know ways to send messages without owls end']\n",
            "['dunno but he says its all important and itll help me survive end']\n",
            "['stupefy stupefy stubefy end']\n",
            "['i know i can love big deal end']\n",
            "['yes end']\n",
            "['i know end']\n",
            "['yeah thats right ive just been crying my eyes out over my dead mum and im just off to do a bit more end']\n",
            "['dont make me feel worse  and he said people from muggle families shouldnt even be allowed in  end']\n",
            "['so you must know loads of magic already horrible  well not all of them my aunt and uncle and cousin are though wish id had three wizard brothers end']\n",
            "['i see myself shaking hands with dumbledore i  ive won the house cup for gryffindor end']\n",
            "['mr ollivander you told you know who that gregorovitch had the elder wand didnt you never mind how i know it you told you know who that gregorovitch had the wand end']\n",
            "['what was it like end']\n",
            "['we need some help end']\n",
            "['yeah thank god end']\n",
            "['i dont know how much power mcgonagalls got over her end']\n",
            "['ill get them end']\n",
            "['what are you up to end']\n",
            "['how then broomsticks end']\n",
            "['was he after the defense against the dark arts job again sir he didnt say end']\n",
            "['last night i thought it was my dad whod conjured my patronus i mean when i saw myself across the lake i thought i was seeing him it was stupid thinking it was him i mean i knew he was dead end']\n",
            "['what end']\n",
            "['if you ask that once more end']\n",
            "['thanks dedalus its really good of you to do this theyre through here my aunt and uncle and cousin end']\n",
            "['whym i with you end']\n",
            "['yeah at the quidditch world cup end']\n",
            "['what end']\n",
            "['im fine end']\n",
            "['oh i  ive got to go to the library got to get some work done end']\n",
            "['good luck three turns whats he talking about what are we supposed to do end']\n",
            "['not bad you end']\n",
            "['itd be a bit more impressive if she hadnt done it about eighty times before but if id dropped dead every time shes told me im going to id be a medical miracle end']\n",
            "['are you okay hagrid end']\n",
            "['i got it back end']\n",
            "['what are those things dyou reckon what things end']\n",
            "['no he was cleverer than you a better wizard a better man end']\n",
            "['hello end']\n",
            "['there is actually sir its about malfoy and snape end']\n",
            "['but  i  all right but  end']\n",
            "['er  hi er  good to see you end']\n",
            "['i think i can tell who the wrong sort are for myself thanks  end']\n",
            "['i dont think so i think its just been knocked out urgh  troll boogers end']\n",
            "['in her office okay lets go end']\n",
            "['nothing i  poked myself in the eye thats all end']\n",
            "['right end']\n",
            "['ah thats the only bit of me dumbledore cares about isnt it my scar end']\n",
            "['i understand you told him about the twin cores you said he just had to borrow another wizards wand but it didnt work mine still beat the borrowed wand do you know why that is end']\n",
            "['reparo end']\n",
            "['yeah fine end']\n",
            "['what end']\n",
            "['wow  thats right i forgot im seventeen accio glasses end']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_inputs)):\n",
        "    print(\"Input Sentence:\", test_inputs[i])\n",
        "    print(\"Model Output:\", test_candidates[i])\n",
        "    print(\"Reference Sentence:\", test_references[i])\n",
        "    print(\"Response time:\",test_times[i])\n",
        "    print(\"---\" * 10)"
      ],
      "metadata": {
        "id": "b83Mc8WuAiGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average BLEU Score: {bleu_score}\")\n",
        "print(f\"Average Response Time: {avg_time} seconds\")"
      ],
      "metadata": {
        "id": "aMmagRmnDi8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "test_data = load_data('/content/drive/My Drive/en_test_set.json')"
      ],
      "metadata": {
        "id": "TUxKLaouYY4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3x2crdpRiXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract harry dialogues\n",
        "def extract_harry_dialogues(data):\n",
        "\n",
        "    conversations = []\n",
        "\n",
        "    # Iterate through each session\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Store the dialogues as tuples of (input, response)\n",
        "                conversations.append((input_text, response_text))\n",
        "\n",
        "    return conversations\n",
        "\n",
        "# Call the function with the path to your JSON file\n",
        "dialogues = extract_harry_dialogues(data)\n",
        "\n",
        "# Example output\n",
        "for input_text, response_text in dialogues[:10]:  # Print first 5 dialogues for example\n",
        "    print(\"Input:\", input_text)\n",
        "    print(\"Response:\", response_text)\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "id": "mKOt6Tw_axp7",
        "outputId": "ebdad309-cdd5-4c86-e533-74921cd628e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Up! Get up! Now! Up! Up! Are you up yet?\n",
            "Response: Nearly,\n",
            "------\n",
            "Input: Well, get a move on, I want you to look after the bacon. What did you say?\n",
            "Response: Nothing, nothing . . .\n",
            "------\n",
            "Input: On vacation in Majorca,\n",
            "Response: You could just leave me here,\n",
            "------\n",
            "Input: And come back and find the house in ruins? \n",
            "Response: I wont blow up the house,\n",
            "------\n",
            "Input: . . . roaring along like maniacs, the young hoodlums,\n",
            "Response: I had a dream about a motorcycle, It was flying.\n",
            "------\n",
            "Input: MOTORCYCLES DONT FLY!\n",
            "Response: I know they dont, It was only a dream.\n",
            "------\n",
            "Input: Your new school uniform,\n",
            "Response: Oh, I didnt realize it had to be so wet.\n",
            "------\n",
            "Input: Make Harry get it.\n",
            "Response: Make Dudley get it.\n",
            "------\n",
            "Input: Poke him with your Smelting stick,\n",
            "Response: Thats mine!\n",
            "------\n",
            "Input: I want to read that letter,\n",
            "Response: I want to read it, as its mine.\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNvmyxirRhyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}