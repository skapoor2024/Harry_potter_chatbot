{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ervjSv-TpTfc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Attention, TimeDistributed,LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "3QvJkpbetnE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "eOWvE5xPD22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "XriXP9MEfUxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "e531SsBzb8Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_glove_embeddings(path, word_index, embedding_dim=100):\n",
        "    embeddings_index = {}\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector  # words not found in the embedding index will be all-zeros.\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "pk2N_gCJpfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Extract dialogues and preprocess\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Append the preprocessed lines to the lists\n",
        "                inputs.append(input_text)\n",
        "                targets.append('<start>' + response_text + '<end>')\n",
        "\n",
        "    #print(\"Sample input with tokens:\", inputs[0]\n",
        "    #print(\"Sample target with tokens:\", targets[0])\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(inputs + targets)\n",
        "    if '<start>' not in tokenizer.word_index:\n",
        "        tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
        "    if '<end>' not in tokenizer.word_index:\n",
        "        tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n",
        "    print(\"Index of '<start>':\", tokenizer.word_index.get('<start>'))\n",
        "    print(\"Index of '<end>':\", tokenizer.word_index.get('<end>'))\n",
        "    input_seqs = tokenizer.texts_to_sequences(inputs)\n",
        "    target_seqs = tokenizer.texts_to_sequences(targets)\n",
        "    max_len = max(max([len(seq) for seq in input_seqs]), max([len(seq) for seq in target_seqs]))\n",
        "    encoder_input_data = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
        "    decoder_input_data = pad_sequences(target_seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    num_classes = len(tokenizer.word_index) + 1\n",
        "    # One-hot encode the target sequences\n",
        "    decoder_target_data = np.zeros((len(decoder_input_data), max_len, num_classes), dtype='float32')\n",
        "    for i in range(len(decoder_input_data)):\n",
        "        for j in range(max_len):\n",
        "            if j < len(decoder_input_data[i]):\n",
        "                decoder_target_data[i, j, decoder_input_data[i][j]] = 1\n",
        "    print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
        "    print(\"Shape of decoder input data:\", decoder_input_data.shape)\n",
        "    print(\"Shape of decoder target data:\", decoder_target_data.shape)\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data, tokenizer, max_len\n"
      ],
      "metadata": {
        "id": "i8WYj9xbpss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_inference(data,tokenizer):\n",
        "    # Extract dialogues and preprocess\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Append the preprocessed lines to the lists\n",
        "                inputs.append(input_text)\n",
        "                targets.append('<start>' + response_text + '<end>')\n",
        "\n",
        "\n",
        "    input_seqs = tokenizer.texts_to_sequences(inputs)\n",
        "    target_seqs = tokenizer.texts_to_sequences(targets)\n",
        "    max_len = max(max([len(seq) for seq in input_seqs]), max([len(seq) for seq in target_seqs]))\n",
        "    encoder_input_data = pad_sequences(input_seqs, maxlen=max_len, padding='post')\n",
        "    decoder_input_data = pad_sequences(target_seqs, maxlen=max_len, padding='post')\n",
        "\n",
        "    num_classes = len(tokenizer.word_index) + 1\n",
        "    # One-hot encode the target sequences\n",
        "    decoder_target_data = np.zeros((len(decoder_input_data), max_len, num_classes), dtype='float32')\n",
        "    for i in range(len(decoder_input_data)):\n",
        "        for j in range(max_len):\n",
        "            if j < len(decoder_input_data[i]):\n",
        "                decoder_target_data[i, j, decoder_input_data[i][j]] = 1\n",
        "    print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
        "    print(\"Shape of decoder input data:\", decoder_input_data.shape)\n",
        "    print(\"Shape of decoder target data:\", decoder_target_data.shape)\n",
        "\n",
        "    return encoder_input_data,decoder_input_data,decoder_target_data, max_len"
      ],
      "metadata": {
        "id": "ZGVye5DBU_FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(encoder_input_data, decoder_input_data, decoder_target_data, batch_size=64):\n",
        "    # Create a TensorFlow dataset object\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\"encoder_inputs\": encoder_input_data, \"decoder_inputs\": decoder_input_data},\n",
        "        decoder_target_data\n",
        "    ))\n",
        "\n",
        "    # Cache the dataset, shuffle, batch, and prefetch to improve performance\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(buffer_size=len(encoder_input_data))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6xPvjhUO2FXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, max_len, embedding_matrix):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,),name = 'encoder_inputs')\n",
        "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
        "\n",
        "    # Two BiLSTM Encoder layers\n",
        "    encoder_bilstm1 = Bidirectional(LSTM(128, return_sequences=True, return_state=True, dropout=0.2))\n",
        "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_bilstm1(encoder_embedding)\n",
        "    state_h1 = Concatenate()([forward_h1, backward_h1])\n",
        "    state_c1 = Concatenate()([forward_c1, backward_c1])\n",
        "\n",
        "    encoder_bilstm2 = Bidirectional(LSTM(128, return_sequences=True, return_state=True, dropout=0.2))\n",
        "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_bilstm2(encoder_outputs1)\n",
        "    state_h2 = Concatenate()([forward_h2, backward_h2])\n",
        "    state_c2 = Concatenate()([forward_c2, backward_c2])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,), name = 'decoder_inputs')\n",
        "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, dropout=0.2)\n",
        "    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=[state_h2, state_c2])\n",
        "\n",
        "    # Self-Attention Layer\n",
        "    attention_layer = Attention()\n",
        "    attention_result = attention_layer([decoder_outputs, encoder_outputs2])\n",
        "\n",
        "    # Concatenate attention input and decoder LSTM output\n",
        "    decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_result])\n",
        "    decoder_concat_input = LayerNormalization()(decoder_concat_input)\n",
        "\n",
        "    # Dense layer for output\n",
        "    decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "VlzBeIlytrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "train_path = '/content/drive/My Drive/en_train_set.json'\n",
        "train_data = load_data(train_path)\n",
        "encoder_input_data, decoder_input_data,decoder_target_data, tokenizer, max_sequence_len = preprocess_data(train_data)\n",
        "train_encoder_input, val_encoder_input, train_decoder_input, val_decoder_input, train_decoder_target, val_decoder_target = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2, random_state=13)\n",
        "# Create training dataset\n",
        "train_dataset = create_dataset(train_encoder_input, train_decoder_input, train_decoder_target, batch_size=8)\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = create_dataset(val_encoder_input, val_decoder_input, val_decoder_target, batch_size=8)"
      ],
      "metadata": {
        "id": "mVDu4Hkzt3nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d308407d-d3fc-485c-b7d1-7682265da8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of '<start>': 8005\n",
            "Index of '<end>': 8006\n",
            "Shape of encoder input data: (4372, 600)\n",
            "Shape of decoder input data: (4372, 600)\n",
            "Shape of decoder target data: (4372, 600, 8007)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = '/content/drive/My Drive/glove.6B.100d.txt'\n",
        "embedding_matrix = get_glove_embeddings(glove_path, tokenizer.word_index)"
      ],
      "metadata": {
        "id": "pOCQFHVSuaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(len(tokenizer.word_index) + 1,max_sequence_len, embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1Q41B9ut4P",
        "outputId": "31d76325-07f7-420c-8f53-381d84871ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, None, 100)            800700    ['encoder_inputs[0][0]']      \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirecti  [(None, None, 256),          234496    ['embedding_4[0][0]']         \n",
            " onal)                        (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirecti  [(None, None, 256),          394240    ['bidirectional_4[0][0]']     \n",
            " onal)                        (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, None, 100)            800700    ['decoder_inputs[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenat  (None, 256)                  0         ['bidirectional_5[0][1]',     \n",
            " e)                                                                  'bidirectional_5[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 256)                  0         ['bidirectional_5[0][2]',     \n",
            " e)                                                                  'bidirectional_5[0][4]']     \n",
            "                                                                                                  \n",
            " lstm_8 (LSTM)               (None, None, 256)            365568    ['embedding_5[0][0]',         \n",
            "                                                                     'concatenate_12[0][0]',      \n",
            "                                                                     'concatenate_13[0][0]']      \n",
            "                                                                                                  \n",
            " attention_2 (Attention)     (None, None, 256)            0         ['lstm_8[0][0]',              \n",
            "                                                                     'bidirectional_5[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenat  (None, None, 512)            0         ['lstm_8[0][0]',              \n",
            " e)                                                                  'attention_2[0][0]']         \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, None, 512)            1024      ['concatenate_14[0][0]']      \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " time_distributed_2 (TimeDi  (None, None, 8007)           4107591   ['layer_normalization_2[0][0]'\n",
            " stributed)                                                         ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6704319 (25.57 MB)\n",
            "Trainable params: 5102919 (19.47 MB)\n",
            "Non-trainable params: 1601400 (6.11 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "qnfUJ2BgfI_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VlhlpIjSVgUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BLEUScoreCallback(Callback):\n",
        "    def __init__(self, tokenizer, encoder_input_train, decoder_target_train, encoder_input_val, decoder_target_val, max_len):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.encoder_input_train = encoder_input_train\n",
        "        self.decoder_target_train = decoder_target_train\n",
        "        self.encoder_input_val = encoder_input_val\n",
        "        self.decoder_target_val = decoder_target_val\n",
        "        self.max_len = max_len\n",
        "        self.start_token_index = tokenizer.word_index['<start>']\n",
        "        self.end_token_index = tokenizer.word_index['<end>']\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_bleu = self.calculate_bleu(self.encoder_input_train, self.decoder_target_train, 'Train')\n",
        "        val_bleu = self.calculate_bleu(self.encoder_input_val, self.decoder_target_val, 'Validation')\n",
        "        print(f'Epoch {epoch + 1}: Train BLEU Score = {train_bleu:.4f}, Validation BLEU Score = {val_bleu:.4f}')\n",
        "\n",
        "    def calculate_bleu(self, encoder_input_data, decoder_target_data, data_type):\n",
        "        total_bleu_score = 0\n",
        "        num_samples = int(len(encoder_input_data)/10)\n",
        "        sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "        for idx in sample_indices:\n",
        "            input_seq = encoder_input_data[idx:idx+1]  # Process one sequence at a time\n",
        "            target_seq = [self.start_token_index]\n",
        "            output_sentence = []\n",
        "\n",
        "            for _ in range(self.max_len):\n",
        "                target_seq_array = np.array([target_seq])\n",
        "                predictions = self.model.predict([input_seq, target_seq_array])\n",
        "                sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "                if sampled_token_index == self.end_token_index:\n",
        "                    break\n",
        "                output_sentence.append(sampled_token_index)\n",
        "                target_seq.append(sampled_token_index)\n",
        "\n",
        "            # Convert sequences to text\n",
        "            reference = self.tokenizer.sequences_to_texts([decoder_target_data[idx]])\n",
        "            candidate = self.tokenizer.sequences_to_texts([output_sentence])\n",
        "            bleu_score = sentence_bleu([reference[0].split()], candidate[0].split())\n",
        "            total_bleu_score += bleu_score\n",
        "\n",
        "        average_bleu_score = total_bleu_score / num_samples\n",
        "        return average_bleu_score"
      ],
      "metadata": {
        "id": "xXu6oJ-jdPgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bleu_callback = BLEUScoreCallback(tokenizer, train_encoder_input, train_decoder_target, val_encoder_input, val_decoder_target, max_sequence_len)\n",
        "model_history = model.fit(train_dataset, epochs=5,validation_data = val_dataset,verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD_2nswjvvE4",
        "outputId": "379484fa-c5fc-496b-dba6-f73db8f2887c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "55/55 [==============================] - 36s 522ms/step - loss: 0.8649 - accuracy: 0.9621 - val_loss: 0.1278 - val_accuracy: 0.9810\n",
            "Epoch 2/5\n",
            "55/55 [==============================] - 27s 491ms/step - loss: 0.1266 - accuracy: 0.9808 - val_loss: 0.1234 - val_accuracy: 0.9822\n",
            "Epoch 3/5\n",
            "55/55 [==============================] - 27s 491ms/step - loss: 0.1250 - accuracy: 0.9812 - val_loss: 0.1170 - val_accuracy: 0.9824\n",
            "Epoch 4/5\n",
            "55/55 [==============================] - 27s 492ms/step - loss: 0.1186 - accuracy: 0.9817 - val_loss: 0.1149 - val_accuracy: 0.9826\n",
            "Epoch 5/5\n",
            "55/55 [==============================] - 27s 492ms/step - loss: 0.1162 - accuracy: 0.9819 - val_loss: 0.1134 - val_accuracy: 0.9829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code to save the model weights in the drive\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/model_glove_bilstm_weights.h5')\n"
      ],
      "metadata": {
        "id": "pjTRVcbTwX0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1soWw_r-8zwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.plot(model_history.history['loss'], label='Training Loss')\n",
        "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(model_history.history['accuracy'], label='Training Accuracy')  # Change 'accuracy' as per your metrics\n",
        "# plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.legend()\n",
        "# plt.title('Accuracy Over Epochs')\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "DV20UacbTRJA",
        "outputId": "bd5506b5-5839-4c80-a490-1f9b50001346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Over Epochs')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "zfseckq1CU9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the test dataset"
      ],
      "metadata": {
        "id": "Wvikcl52YPi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses_and_evaluate_bleu(model, tokenizer, encoder_input_data, decoder_target_data, max_len, start_token_index, end_token_index):\n",
        "    total_bleu_score = 0\n",
        "    num_samples = int(len(encoder_input_data)/100)  # Use the entire dataset\n",
        "    sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "    total_time = 0\n",
        "    response_times = []\n",
        "\n",
        "\n",
        "\n",
        "    all_inputs = []\n",
        "    all_references = []\n",
        "    all_candidates = []\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        input_seq = encoder_input_data[idx:idx+1]  # Extract one sequence for further processing\n",
        "        input_text = tokenizer.sequences_to_texts(input_seq)[0]  # Convert the extracted sequence to text\n",
        "        target_seq = [start_token_index]\n",
        "        output_sentence = []\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"sentence_started: \",input_text)\n",
        "        print(input_text)\n",
        "        print(\"Harry: \",end = ' ')\n",
        "        for _ in range(max_len):\n",
        "            target_seq_array = np.array([target_seq])\n",
        "            predictions = model.predict([input_seq, target_seq_array],verbose = 0)\n",
        "            sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "            if sampled_token_index == end_token_index or len(target_seq) == max_len:\n",
        "                break\n",
        "            sampled_word = tokenizer.index_word[sampled_token_index] if sampled_token_index in tokenizer.index_word else ''\n",
        "            print(sampled_word,end = ' ')  # Print each predicted word\n",
        "            output_sentence.append(sampled_token_index)\n",
        "            target_seq.append(sampled_token_index)\n",
        "\n",
        "        print()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "        response_times.append(elapsed_time)\n",
        "        print(\"time elapsed: \",elapsed_time)\n",
        "\n",
        "        # Convert sequences to text\n",
        "        reference = tokenizer.sequences_to_texts([decoder_target_data[idx]])\n",
        "        candidate = tokenizer.sequences_to_texts([output_sentence])\n",
        "\n",
        "        all_inputs.append(input_text[0].split())\n",
        "        all_references.append(reference[0].split())\n",
        "        all_candidates.append(candidate[0].split())\n",
        "\n",
        "        # Calculate individual BLEU score\n",
        "        bleu_score = sentence_bleu([reference[0].split()], candidate[0].split())\n",
        "        total_bleu_score += bleu_score\n",
        "\n",
        "        # print(\"Input Sentence:\", input_text)\n",
        "        # print(\"Model Output:\", candidate)\n",
        "        # print(\"Reference Sentence:\", reference)\n",
        "        # print(\"---\" * 10)  # Separator for readability\n",
        "        print(\"sentence_finished\")\n",
        "\n",
        "\n",
        "    average_bleu_score = total_bleu_score / num_samples\n",
        "    average_response_time = sum(response_times) / num_samples\n",
        "\n",
        "    return all_inputs,all_references,all_candidates,response_times,average_bleu_score,average_response_time\n"
      ],
      "metadata": {
        "id": "jsQBnqh69txy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = load_data('/content/drive/My Drive/en_test_set.json')\n",
        "encoder_input_data, decoder_input_data, decoder_target_data, max_len = preprocess_data_inference(test_data,tokenizer)"
      ],
      "metadata": {
        "id": "Yp9KwFVKYfM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_responses(model, tokenizer, encoder_input_data, max_len, start_token_index, end_token_index):\n",
        "    num_samples = len(encoder_input_data)# Reduce the dataset for quicker response times\n",
        "    sample_indices = np.random.choice(len(encoder_input_data), num_samples, replace=False)\n",
        "\n",
        "    total_time = 0\n",
        "    response_times = []\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        input_seq = encoder_input_data[idx:idx+1]  # Extract one sequence for further processing\n",
        "        input_text = tokenizer.sequences_to_texts(input_seq)[0]  # Convert the extracted sequence to text\n",
        "\n",
        "        print(\"Input: \", input_text)\n",
        "        print(\"Harry: \", end=' ')\n",
        "\n",
        "        target_seq = [start_token_index]\n",
        "        output_sentence = []\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        while len(target_seq) < max_len:\n",
        "            target_seq_array = np.array([target_seq])\n",
        "            predictions = model.predict([input_seq, target_seq_array], verbose=0)\n",
        "            sampled_token_index = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "            if sampled_token_index == end_token_index or len(target_seq) == max_len:\n",
        "                break\n",
        "\n",
        "            sampled_word = tokenizer.index_word.get(sampled_token_index, '')\n",
        "            print(sampled_word, end=' ')  # Print each predicted word on the same line\n",
        "            output_sentence.append(sampled_token_index)\n",
        "            target_seq.append(sampled_token_index)\n",
        "\n",
        "        print()  # New line after the end of response\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "        response_times.append(elapsed_time)\n",
        "\n",
        "        print(\"Time elapsed for response: {:.4f} seconds\".format(elapsed_time))\n",
        "        print(\"---\" * 20)  # Separator for readability\n",
        "\n",
        "    average_response_time = total_time / num_samples\n",
        "\n",
        "    return average_response_time, num_samples\n"
      ],
      "metadata": {
        "id": "ylTkuECQXwHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "average_response_time, num_samples = generate_responses(\n",
        "    model, tokenizer, encoder_input_data, 100,\n",
        "    tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
        ")\n",
        "print(\"Processed {} sentences with an average response time of {:.4f} seconds.\".format(num_samples, average_response_time))"
      ],
      "metadata": {
        "id": "y7x5QyujX_DJ",
        "outputId": "e5c0e9dc-f930-4922-842c-ad0d3d7a4530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  are you all right you look awful\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 10.9827 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  nah that just one of those things you tell kids to teach them lessons isn’t it ‘don’t go looking for trouble don’t pick fights don’t go messing around with stuff that’s best left alone just keep your head down mind your own business and you’ll be okay ’ come to think of it maybe that why elder wands are supposed to be unlucky\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3497 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  good afternoon\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3134 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  merry christmas\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.3315 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  you were yelling your head off\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.2997 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  stan shunpike\n",
            "Harry:  i end                                                                                                  \n",
            "Time elapsed for response: 7.2816 seconds\n",
            "------------------------------------------------------------\n",
            "Input:  i think that if you choose to return there is a chance that he may be finished for good i cannot promise it but i know this harry that you have less to fear from returning here than he does harry pity the living and above all those who live without love by returning you may ensure that souls are maimed families are torn apart if that seems to you a worthy goal then we say good bye for the present\n",
            "Harry:  i end                                                 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-913413c046b4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m average_response_time, num_samples = generate_responses(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-41-268f67212800>\u001b[0m in \u001b[0;36mgenerate_responses\u001b[0;34m(model, tokenizer, encoder_input_data, max_len, start_token_index, end_token_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtarget_seq_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_truncate_execution_to_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         should_truncate = (\n\u001b[1;32m   1360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1363\u001b[0m         \u001b[0moriginal_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    691\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \"\"\"\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    809\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    810\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    532\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    535\u001b[0m         _ctx, \"ReadVariableOp\", name, resource, \"dtype\", dtype)\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs,test_references,test_candidates,test_times,bleu_score,avg_time = generate_responses_and_evaluate_bleu(\n",
        "    model, tokenizer, encoder_input_data, decoder_target_data, max_len,\n",
        "    tokenizer.word_index['<start>'], tokenizer.word_index['<end>']\n",
        ")"
      ],
      "metadata": {
        "id": "riMAam1u-0Yr",
        "outputId": "c70150e1-7511-431b-c309-06a137adfa14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_started:  gone\n",
            "gone\n",
            "Harry:  yeah end                                                                                                                                                                                                                   \n",
            "time elapsed:  19.294798374176025\n",
            "sentence_finished\n",
            "sentence_started:  third time this week if you can’t control that owl it’ll have to go\n",
            "third time this week if you can’t control that owl it’ll have to go\n",
            "Harry:  yeah "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end                                                                                                                                                                                                                   \n",
            "time elapsed:  15.826366424560547\n",
            "sentence_finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(len(train_decoder_target), 100)):  # Ensure not to exceed the length of the data or 100 items\n",
        "    # Ensure we pass a list of sequences, even if it's a list with only one sequence\n",
        "    sequence_text = tokenizer.sequences_to_texts([val_decoder_target[i]])\n",
        "    print(sequence_text)  # This will print the list containing the text representation of the i-th sequence\n"
      ],
      "metadata": {
        "id": "eAI17_zSKCoF",
        "outputId": "e091ab94-872e-46f3-f63c-95a9904e79e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['where is it now end']\n",
            "['what about dumbledore hagrid end']\n",
            "['“what ” end']\n",
            "['yeah i do malfoy’s father’s in azkaban don’t you think malfoy’d like revenge end']\n",
            "['fleur didn’t turn up i couldn’t leave her end']\n",
            "['but if voldemort’s trying to recruit more death eaters it’s bound to get out that he’s come back isn’t it end']\n",
            "['there were witnesses who saw pettigrew die a whole street full of them end']\n",
            "['oh i dunno let’s say i dreamed i was drowning snape in my cauldron yeah that’ll do end']\n",
            "['yeah i expect that’s what mcgonagall will say when i ask for permission end']\n",
            "['less than zero better try though hadn’t i i’ll offer to do two more detentions or something i dunno i hope she doesn’t keep me too long this evening you realize we’ve got to write three essays practice vanishing spells for mcgonagall work out a countercharm for flitwick finish the bowtruckle drawing and start that stupid dream diary for trelawney end']\n",
            "['fortuna major end']\n",
            "['“well hurry up i can’t breathe ” end']\n",
            "['where are we going end']\n",
            "['— i meant to and that’s what did it i’ve done what my mother did they’re protected from you haven’t you noticed how none of the spells you put on them are binding you can’t torture them you can’t touch them you don’t learn from your mistakes riddle do you end']\n",
            "['so you think that dream did it really happen end']\n",
            "['kind of makes you wish we had norbert back doesn’t it end']\n",
            "['but the fake sword isn’t the only thing in that vault is it perhaps you’ve seen the other things in there end']\n",
            "['it wasn’t like that end']\n",
            "['but how did you get in there you need to speak parseltongue end']\n",
            "['i need you two as well end']\n",
            "['dumbledore just said — just said we could save more than one innocent life hermione we’re going to save buckbeak end']\n",
            "['looks like a really good plan from where i’m standing they’ll just have to stand back and let the dragons do their stuff end']\n",
            "['have you been spying on him too end']\n",
            "['cho women what did she want to talk about cedric for anyway why does she always want to drag up a subject that makes her act like a human hosepipe end']\n",
            "['it looks like king’s cross station except a lot cleaner and empty and there are no trains as far as i can see end']\n",
            "['no it definitely didn’t sound like an elf end']\n",
            "['he’s not looking too good is he end']\n",
            "['and do you — end']\n",
            "['double divination this afternoon end']\n",
            "['the snitch i caught in my first ever quidditch match don’t you remember end']\n",
            "['no no end']\n",
            "['like voldemort put on the stone basin in the cave end']\n",
            "['well i don’t know what they are but apparently he and his daughter go on holiday looking for them that’s her end']\n",
            "['neville end']\n",
            "['“ron —” end']\n",
            "['no you didn’t you didn’t do that you can’t have done end']\n",
            "['think of a dream quick in case the old toad comes our way end']\n",
            "['what’s dumbledore asked you to do hagrid he sent professor mcgonagall to ask you and madame maxime to meet him — that night end']\n",
            "['you considered smith end']\n",
            "['okay end']\n",
            "['i dunno maybe it’s better when you do it yourself i didn’t enjoy it much when dumbledore took me along for the ride charlie failed though didn’t he end']\n",
            "['what did you tell her end']\n",
            "['he’s friends with that dog i’ve seen them together come on — and keep your wand out — end']\n",
            "['yeah i want to play quidditch hang on i’ll get my firebolt end']\n",
            "['unless he was asleep said harry but he still held his breath as hermione knelt down in front of the empty canvas her wand directed at its center cleared her throat then said “er — phineas phineas nigellus ” end']\n",
            "['they brought the kids from the orphanage here end']\n",
            "['why d’you wear that thing dobby end']\n",
            "['hagrid thanks end']\n",
            "['well i’m glad he left if he hadn’t i wouldn’t have done magic and dumbledore would probably have left me at privet drive all summer end']\n",
            "['i hoped he’d get back to me quickly end']\n",
            "['he could still’ve kept me informed if he’d wanted to you’re not telling me he doesn’t know ways to send messages without owls end']\n",
            "['dunno but he says it’s all important and it’ll help me survive end']\n",
            "['stupefy stupefy stubefy end']\n",
            "['i know i can love big deal end']\n",
            "['yes end']\n",
            "['i know end']\n",
            "['yeah that’s right i’ve just been crying my eyes out over my dead mum and i’m just off to do a bit more end']\n",
            "['don’t make me feel worse — and he said people from muggle families shouldn’t even be allowed in — end']\n",
            "['so you must know loads of magic already horrible — well not all of them my aunt and uncle and cousin are though wish i’d had three wizard brothers end']\n",
            "['i see myself shaking hands with dumbledore i — i’ve won the house cup for gryffindor end']\n",
            "['mr ollivander you told you know who that gregorovitch had the elder wand didn’t you never mind how i know it you told you know who that gregorovitch had the wand end']\n",
            "['what was it like end']\n",
            "['we need some help end']\n",
            "['yeah thank god end']\n",
            "['i don’t know how much power mcgonagall’s got over her end']\n",
            "['i’ll get them end']\n",
            "['what are you up to end']\n",
            "['how then broomsticks end']\n",
            "['was he after the defense against the dark arts job again sir he didn’t say end']\n",
            "['last night i thought it was my dad who’d conjured my patronus i mean when i saw myself across the lake i thought i was seeing him it was stupid thinking it was him i mean i knew he was dead end']\n",
            "['“what end']\n",
            "['if you ask that once more end']\n",
            "['thanks dedalus it’s really good of you to do this they’re through here my aunt and uncle and cousin end']\n",
            "['why’m i with you end']\n",
            "['yeah at the quidditch world cup end']\n",
            "['what end']\n",
            "['i’m fine end']\n",
            "['oh i — i’ve got to go to the library got to get some work done end']\n",
            "['good luck three turns what’s he talking about what are we supposed to do end']\n",
            "['not bad you end']\n",
            "['it’d be a bit more impressive if she hadn’t done it about eighty times before but if i’d dropped dead every time she’s told me i’m going to i’d be a medical miracle end']\n",
            "['are you okay hagrid end']\n",
            "['i got it back end']\n",
            "['what are those things d’you reckon what things end']\n",
            "['no he was cleverer than you a better wizard a better man end']\n",
            "['hello end']\n",
            "['there is actually sir it’s about malfoy and snape end']\n",
            "['but — i — all right but — end']\n",
            "['er — hi er — good to see you end']\n",
            "['“i think i can tell who the wrong sort are for myself thanks ” end']\n",
            "['i don’t think so i think it’s just been knocked out urgh – troll boogers end']\n",
            "['in her office okay let’s go end']\n",
            "['nothing i — poked myself in the eye that’s all end']\n",
            "['right end']\n",
            "['ah that’s the only bit of me dumbledore cares about isn’t it my scar end']\n",
            "['i understand you told him about the twin cores you said he just had to borrow another wizard’s wand but it didn’t work mine still beat the borrowed wand do you know why that is end']\n",
            "['reparo end']\n",
            "['yeah fine end']\n",
            "['what end']\n",
            "['wow — that’s right i forgot i’m seventeen accio glasses end']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_inputs)):\n",
        "    print(\"Input Sentence:\", test_inputs[i])\n",
        "    print(\"Model Output:\", test_candidates[i])\n",
        "    print(\"Reference Sentence:\", test_references[i])\n",
        "    print(\"Response time:\",test_times[i])\n",
        "    print(\"---\" * 10)"
      ],
      "metadata": {
        "id": "b83Mc8WuAiGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average BLEU Score: {bleu_score}\")\n",
        "print(f\"Average Response Time: {avg_time} seconds\")"
      ],
      "metadata": {
        "id": "aMmagRmnDi8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "test_data = load_data('/content/drive/My Drive/en_test_set.json')"
      ],
      "metadata": {
        "id": "TUxKLaouYY4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3x2crdpRiXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract harry dialogues\n",
        "def extract_harry_dialogues(data):\n",
        "\n",
        "    conversations = []\n",
        "\n",
        "    # Iterate through each session\n",
        "    for session_id, session in data.items():\n",
        "        dialogue = session['dialogue']\n",
        "        for i in range(len(dialogue) - 1):  # Ensure there's a following line to consider as a response\n",
        "            if \"Harry:\" not in dialogue[i] and \"Harry:\" in dialogue[i + 1]:\n",
        "                # Extract the line before Harry's response if Harry is not speaking in the current line\n",
        "                input_line = dialogue[i]\n",
        "                response_line = dialogue[i + 1]\n",
        "\n",
        "                # Parse the lines to remove the speaker names\n",
        "                input_text = input_line.split(': ', 1)[1] if ': ' in input_line else input_line\n",
        "                response_text = response_line.split(': ', 1)[1] if ': ' in response_line else response_line\n",
        "\n",
        "                # Store the dialogues as tuples of (input, response)\n",
        "                conversations.append((input_text, response_text))\n",
        "\n",
        "    return conversations\n",
        "\n",
        "# Call the function with the path to your JSON file\n",
        "dialogues = extract_harry_dialogues(data)\n",
        "\n",
        "# Example output\n",
        "for input_text, response_text in dialogues[:10]:  # Print first 5 dialogues for example\n",
        "    print(\"Input:\", input_text)\n",
        "    print(\"Response:\", response_text)\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "id": "mKOt6Tw_axp7",
        "outputId": "ebdad309-cdd5-4c86-e533-74921cd628e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Up! Get up! Now! Up! Up! Are you up yet?\n",
            "Response: Nearly,\n",
            "------\n",
            "Input: Well, get a move on, I want you to look after the bacon. What did you say?\n",
            "Response: Nothing, nothing . . .\n",
            "------\n",
            "Input: On vacation in Majorca,\n",
            "Response: You could just leave me here,\n",
            "------\n",
            "Input: And come back and find the house in ruins? \n",
            "Response: I won’t blow up the house,\n",
            "------\n",
            "Input: . . . roaring along like maniacs, the young hoodlums,\n",
            "Response: I had a dream about a motorcycle, It was flying.\n",
            "------\n",
            "Input: MOTORCYCLES DON’T FLY!\n",
            "Response: I know they don’t, It was only a dream.\n",
            "------\n",
            "Input: Your new school uniform,\n",
            "Response: Oh, I didn’t realize it had to be so wet.\n",
            "------\n",
            "Input: Make Harry get it.\n",
            "Response: Make Dudley get it.\n",
            "------\n",
            "Input: Poke him with your Smelting stick,\n",
            "Response: That’s mine!\n",
            "------\n",
            "Input: I want to read that letter,\n",
            "Response: I want to read it, as it’s mine.\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNvmyxirRhyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}